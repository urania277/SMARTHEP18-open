%From introduction to be added here

%Data transfer and storage are expensive for research data centers as well as Internet-of-Things (IoT) devices\footnote{Dataversity, \href{https://www.dataversity.net/internet-things-vs-edge-computing-processing-real-time-data/}{Internet of Things vs. Edge Computing: Processing Real-Time Data}, accessed online 12/2018}.

%(including inter/multidisciplinary, intersectoral and, where appropriate, gender aspects)

\subsubsection{Introduction, objectives and overview of the research program}
\label{sec:introRO}

%\noindent{\color{blue}{State of the art and how \acronym will go beyond it}.}
%brief description, international bibliographic references, cite the consortium

Researchers in HEP study the basic constituents of matter and their interactions within the theory called the Standard Model (SM), and search for new physics phenomena explaining the SM's shortcomings. 
One of the main experimental tools available for this study is the LHC. 
At the LHC, particle beams are accelerated to high energies and collided every 25 ns. 
The products of the collisions are analyzed by scientists operating four main particle detectors: ALICE, ATLAS, CMS and LHCb\footnote{CERN, \href{http://cds.cern.ch/record/1997374}{The LHC Experiments}, CERN Document Server}.
Collision \textit{events} occur 30 million times per second. 
Recording each event would be prohibitive for the current data acquisition and storage systems. 
Since many of those events contain known processes that are not considered interesting, each experiment has a \textit{trigger system} composed of hardware and software.
The trigger system analyzes collision events in real time, with a maximum delay of micro- to milliseconds, and selects which ones to keep for further analysis. 
This real time event selection is crucial for HEP, especially after the LHC second running period (Run-2) has drawn to a close. 
While there are many observed phenomena that are still not included in the SM (e.g. dark matter [LINK BOVEIA DOGLIONI ANN REV]), the leading predictive theories such as Supersymmetry have so far yielded null results. 
This is prompting HEP researchers to reconsider their definition of "interesting". 
New phenomena could have unexpected, not-yet-theorized manifestations in the detector, and they would be missed using the current trigger algorithms. 
New phenomena could also be extremely rare and buried in uninteresting backgrounds, and require specialized data taking techniques. 
Moreover, disagreements of data with the SM prediction require dedicated investigation with the new dataset, and therefore dedicated trigger reconstruction and selection criteria. 
For example, the SM prediction that the weak force couplings to all lepton types are equal (Lepton Flavor Universality, or LFU) has recently been challenged by LHCb measurements~\footnote{LHCb Collaboration, \href{https://arxiv.org/abs/1705.05802}{Test of lepton universality with $ B^{+}\rightarrow K^{+}\ell^{+}\ell$}, Phys. Rev. Lett. 113, 151601 (2014). LHCb Collaboration, \href{https://arxiv.org/abs/1705.05802}{Test of lepton universality with $B^{0}\rightarrow K^{* 0}\ell^{+}\ell$}, JHEP 08 (2017) 055.}. This makes LFU and LFV (Lepton Flavor Violation, the prediction that the overall number of leptons of a given type does not change in interactions) among the most interesting topics for the near-future of particle physics and one that needs RTA to be explored. 
%Studying these processes with Run-3 data requires the implementation of dedicated trigger reconstruction and selection criteria. 

As detailed in Secs.~\ref{sec:trainingcontrib} and~\ref{ss:competence_44}, researchers in the \acronym consortium are experts on the trigger systems of all four main LHC experiments. 
Within \acronym, they will pool their expertise and complement it with that of computer scientists, entrepreneurs and professionals.\\
\fbox{\begin{minipage}{0.985\textwidth}
\textbf{\acronym will train a new generation of researchers with a strong expertise on RTA techniques and analysis of large datasets. The ESRs will exploit innovative ML techniques and hybrid computing architectures to upgrade the trigger systems of LHC experiments for the next LHC data taking periods where the volume of data will increase even further. They will perform physics analysis with these improved systems, towards precise probes of the SM and searches for new phenomena.} 
\end{minipage}}

\acronym's ESR projects employ RTA in research as well as in projects that are useful to industry and society. 
Time-to-insight from raw data is a crucial benchmark for a competitive and effective use of the information. For this reason, advancement in terms of fast and efficient data analysis and decision-making are required by society and the commercial sector, and a selection of industrial use cases sharing common issues with HEP has been chosen as integral part of the \acronym research and exploitation program as described further in Sec.~\ref{sec:exploit}.  
RTA is a key point for improving transport and its safety, as well as for optimizing industrial processes. 
The information on the position of private or public transport vehicles and their possible routes are transmitted to a central analysis system, so that the traffic conditions around it are analyzed and a forecast is made. 
All of this needs to be done on a timescale that is short enough for a city to modify its traffic light system to avoid congestion, or to reroute other vehicles to avoid delays. 
This requires both ML and hybrid computing architectures for algorithm training, similarly to the solution of complex problems such as tracking particles in a HEP detector. 
The detection of a driver's behavior (e.g. drowsiness) in fleets of cars needs to happen in the timescale of milliseconds to preserve safety, and detection of malfunctioning components in industrial processes must trigger maintenance before the component breaks. Both those use cases require the optimization of advanced algorithms on limited computational resources, much like HEP's trigger systems.
Another use case for RTA in industry is fraud detection, where the detection of an anomalous transaction and a decision on whether it is allowed or not needs to be made on a short timescale. This is a similar challenge to HEP triggers looking for unexpected phenomena. 
While ML algorithms trained to detect outliers are the current state of the art for this purpose, their black box approach raises interpretability and explainability challenges. 
This is one of the topics beyond the state-of-the-art that is tackled by \acronym: knowledge-based approaches for the induction of explicit decision rules are useful both for increasing the accountability of fraud detection algorithms and for HEP trigger systems where every decision to discard an event is final and must be completely understood.\\ 
\fbox{\begin{minipage}{0.985\textwidth}
\textbf{The cross-talk on RTA between academia and industry in \acronym will lead to concrete deliverables that contribute to improving European transport as per the \href{https://ec.europa.eu/programmes/horizon2020/en/h2020-section/smart-green-and-integrated-transport}{H2020 Societal Challenge for Smart, green and integrated transport}, to enhancing the capabilities and reproducibility of decision-making in finance and to improving industrial processes.}
\end{minipage}}

\subsubsection*{Consortium composition}

\acronym spans ten different countries. 
\cernentity, the \nikhefentity and \cnrsentity research organizations, and universities \sorbonneentity, \helsinkientity, \unigeentity, \dortmundentity, \lundentity, \heidelbergentity form the core academic side of the Network, with experts in HEP and computer science.
\ibmentity and \fleetmaticsentity as beneficiaries, and \ximantisentity, \lightboxentity, \pointeightentity, as partners, form a balanced counterpart to academia with a focus on the same technological challenges as HEP to gain insight into data in real-time. 
\oregonentity, \ohioentity, \pisaentity, \santiagoentity, \liegesentity, \radboudentity and \amsterdamentity are high-profile associated academic partners whose expertise reinforces the network, that provide training and secondment to the ESRs, and award PhD degrees in case the beneficiaries are industries and research institutes\footnote{In this proposal, we use "node" to indicate
either beneficiary or partner. The \textit{node responsible} is the \textit{main scientific contact person} for the node. Administrators, albeit indispensable for the network functioning, are not named even if they are node responsibles in the case scientists are affiliated to two nodes.}.
All consortium members have successfully proven their capabilities for research, training, exploitation and dissemination as shown in Sec.~\ref{sec:supervision},~\ref{ss:competence_44} and part B2.

\subsubsection*{Network goals and research objectives}

\acronym trains researchers in ML, AI and data analysis and hybrid computing architectures, in order to enable RTA for the advancement of decision-making as well as monitoring and discoveries in both research and industry. 
The work and expertise developed within \acronym will develop and commission upgraded triggers systems for LHC experiments for HEP measurements and searches, improve transport, industrial process and financial decision-making. 
These network goals define the four main research topics for \acronym, each corresponding to a research Work Package (WP) detailed in Sec.~\ref{sec:metho}. 
Each of the research topics has research objectives (RO) shown in each table. The RO can be achieved with this network thanks to the collaboration between industry and HEP, and they are linked to concrete deliverables. 

%\vspace{-2mm}
%\begin{multicols}{2}[]
%%\small
%\begin{enumerate}%{\leftmargin=1em}

The first research topic concerns the use of \textbf{ML, AI and advanced data analysis} in RTA techniques.
%The sheer size of the datasets collected by scientific experiments has lead researchers in HEP to use advanced data analysis methods normally used in industry, such as ML and AI. 
ML techniques are by now ubiquitous in both HEP and industry, in order to optimally analyze raw, unstructured data. 
In \acronym we will further extend their use in RTA and event reconstruction, paying particular attention to their understandability and reproducibility (see research topic 4).   
Meta-analysis (benchmarking) is also necessary to understand the algorithm performance prior to using them in RTA and to satisfy he requirements of resource-constrained environments.

\begin{center}
\vskip-15pt
\resizebox {\textwidth }{!}{%
\small
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
%\toprule
\hline
\textbf{Research Objective:} & \textbf{Outcomes:} \\
\hline
design and use a variety of ML algorithms for RTA (GAN, RNN, Deep Learning)  & HEP and commercial software toolkits (ADD DELIVERABLES);\\
\hline
implement efficient real-time object and event reconstruction & software for HEP experiment triggers (ADD DELIVERABLES);\\
\hline
benchmark and optimize RTA algorithm performance & inter-sector benchmarking toolkits (ADD DELIVERABLES);\\
\hline
%\bottomrule
%\vspace{-1cm}
\end{tabular}
}
\end{center}
%\end{table}
%\vskip5pt

The second research topic focuses on the design of innovative \textbf{hybrid computing architecture} solutions in hardware/software, given that the complexity and rate of LHC data and beyond does not allow standard processors or data analysis techniques to be competitive\footnote{J.P. Vlimant, \href{https://erez.weizmann.ac.il/pls/htmldb/f?p=101:58:::NO:RP:P58_CODE,P58_FILE:5393,Y}{Machine Learning in Charged Particle Tracking}, Hammers and Nails ML\&HEP Conference, 2017}. An example of such a technology in HEP is the Fast TracKer in ATLAS, a hardware-based ATLAS Fast TracKer (FTK)\footnote{J Adelman et al., \href{https://inspirehep.net/record/1614024/}{ATLAS FTK Challenge: Simulation of a Billion-fold Hardware Parallelism,} Procedia Comput.Sci. 66 (2015) 540-545}, a unique hardware processor made by several custom electronic boards based on Field Programmable Gate Arrays (FPGA) to reconstruct the trajectory (track) of the charged particles that cross the inner part of the experiment. The FTK is a complex system made by several custom electronics boards based on FPGAs and unique computing devices. In industry, Graphic Processing Units (GPU) are used for computing parallel processes and training ML algorithms, and their use is being prototyped in HEP by members of \acronym. 
\begin{center}
\vskip-15pt
\resizebox {\textwidth }{!}{%
\small
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
%\toprule
\hline
\textbf{Research Objective:} & \textbf{Outcomes:} \\
\hline
use of FPGAs for fast hardware computations & trigger improvements for LHC experiments (ADD DELIVERABLES);\\
\hline
use of GPUs for massively parallel computations & significant speed-up for HEP and industrial ML and RTA techniques (ADD DELIVERABLES);\\
\hline
design parallelized and multithreaded algorithms & RTA software needed for the increase in LHC data and for more efficient financial transactions (ADD DELIVERABLES);\\
\hline
%\bottomrule
%\vspace{-1cm}
\end{tabular}
}
\end{center}
%\end{table}
%\vskip5pt

The third research topic concerns \textbf{real-time decision making in physics and industry}. 
The outcomes of the previous research topics will be applied to concrete physics cases and industrial challenges. 
These improvements in RTA benefit each experiment's physics program and increase the information quality extracted from data-rich environments in industry.  
There are many HEP drivers behind the trigger improvements that will be the \acronym ESR's thesis topics: hints of physics beyond the SM in LFV/LFU, the mystery of 85\% of the matter content of the universe that is not yet included in the SM, the in-depth study of the newly discovered Higgs boson and the study of collective particle behavior from states of matter created shortly after the Big Bang. Similar techniques used for trigger improvements will also help improve transport and fleet safety. 
\begin{center}
\vskip-15pt
\resizebox {\textwidth }{!}{%
\small
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
%\toprule
\hline
\textbf{Research Objective:} & \textbf{Outcomes:} \\
\hline
deploy improvements from real-time reconstruction and hybrid architectures in HEP trigger systems & measurements and searches with LHC Run-3 data (ADD DELIVERABLES) ;\\
\hline
use ML and hybrid architecture for RTA to improve transport and safety of fleets & inputs to traffic management of connected cities, extended transport apps and in-vehicle software (ADD DELIVERABLES);\\
\hline
%\bottomrule
%\vspace{-1cm}
\end{tabular}
}
\end{center}
%\end{table}
%\vskip5pt

The fourth research topic concerns efficient and accountable \textbf{monitoring} of complex processes and increase the sensitivity to physics \textbf{discoveries} through outlier detection and novel RTA techniques in data taking. 
Firstly, industry and HEP share the challenge of not knowing in advance what kind of "interesting" events will appear, in the case of malfunctioning equipment, fraudulent financial transaction or potential not-yet-theorised new physics process. 
Outlier/novelty detection is a growing field, with dedicated ML algorithms\footnote{R. Domingues et al, \href{A comparative evaluation of outlier detection algorithms: experiments and analyses}{http://doi.org/10.1016/j.patcog.2017.09.037}, Pattern Recognition 74 (2018)}. In \acronym, experts in the field from \ibmentity and their supervised ESRs will apply those to physics and industry use cases, and extending them to methods where rules on what are inferred from a mixture of theory and known events to increase their accountability and understandability. 
Secondly, the present HEP paradigm, in which a pre-selection of interesting signals based on simple event features reduces the event rate to a manageable size for further storage, works well in an environment dominated by uninteresting backgrounds. 
%, where the backgrounds can be distinguished from the signal using those event features (e.g. only around one in ten trillion LHC collisions produce a Higgs boson). 
However, crucial HEP research questions cannot be answered entirely by physics processes with these characteristics. Not being able to fully exploit the wealth of collision data delivered by the LHC because it is impossible to save it for further analysis is a severe cost for the advancement of the physics program of all LHC experiments, especially in light of the planned upgrades to the LHC collider that will significantly increase the collision rates. For this reason, many of the members of \acronym have been extending the physics program of their experiments using novel RTA techniques in data taking\footnote{e.g. Gligorov and Fitzpatrick, \href{http://cds.cern.ch/record/1670985}{Anatomy of an upgrade event in the upgrade era, and implications for the LHCb trigger}, CERN Document Server. ALICE Collaboration including Christiansen, \href{https://cds.cern.ch/record/2011297/files/ALICE-TDR-019.pdf}{Technical Design Report}, CDS Document Server, ADD ATLAS TRIGGER PAPER}. 
The technique they pioneered exploits real-time reconstruction and analysis at the trigger level to only record high-level  information, rather than the raw detector information as it is traditionally done. 
Since high-level quantities are much smaller than raw data, more events can be recorded and used for physics analysis, and and a much increased sensitivity for measurements and discoveries can be achieved\footnote{CMS Collaboration including Pierini, \href{https://arxiv.org/abs/1604.08907}{Search for narrow resonances in dijet final states at $\sqrt{s}$ = 8 TeV with the novel CMS technique of data scouting}, Phys. Rev. Lett. 117, 031802 (2016). ATLAS Collaboration including Boveia, Starovoitov, Dunford and Strom, ADD HERE TLA REF, LHCb collaboration including Gligorov, Borsato ADD HERE DARK PHOTON REF}. 
\acronym will improve and extend this technique beyond the current state-of-the-art towards the upgrades, in LHCb where most of the data will be recorded this way, in ALICE where it will be implemented for the first time, and in ATLAS and CMS where it will be applied to a larger number of searches. 
\begin{center}
\vskip-15pt
\resizebox {\textwidth }{!}{%
\small
\begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
%\toprule
\hline
\textbf{Research Objective:} & \textbf{Outcomes:} \\
\hline
implement outlier detection techniques in physics and industry & potential for not-yet-theorized discoveries at the LHC, improved fraud detection algorithms and IoT-ready industrial predictive maintenance (ADD DELIVERABLES) ;\\
\hline
study and design rule-induction algorithms as complement to ML to improve accountability and understandability & RTA for fraud detection and HEP triggers (ADD DELIVERABLES);\\
\hline
extend RTA data-taking techniques in HEP trigger systems where raw data is discarded & increased sensitivity to new physics processes in LFV/LFU, Higgs boson measurements and dark matter searches\\
\hline
%\bottomrule
%\vspace{-1cm}
\end{tabular}
}
\end{center}
%\end{table}
%\vskip5pt


\subsubsection{Research methodology and approach}
\label{sec:metho}

%one-paragraph summary of each of the WPs (10-12 lines)
%include how this will be explored (credibility/feasibility here)
%highlight inter/multidisciplinary 
%Explain how the individual ESRs fit into the WP in table 
%use a figure to illustrate relationship?

%\multicolumn{8}{p{0.95\textwidth}}{
%ESRs: \ESRb, \ESRc, \ESRf, \ESRg, \ESRl, \ESRm, \ESRn
%The implementation of ML and detector reconstruction algorithms will be tested on GPUs by ESRs 3, 9, 10 and 11. 
%The ATLAS Fast TracKer (FTK) (see ATLAS Collaboration,\href{https://inspirehep.net/record/1614024/}{The ATLAS Fast TracKer}, CERN Document Server, 2016), a hardware solution to particle track reconstruction will be the focus of ESRs 4, 8 and 15. ESR10 will optimize data formats and processing techniques to enable CPUs, GPUs, FPGAs, and hybrids to work together to solve problems which none of these technologies could solve on their own. ESR4 and ESR8 will investigate parallelization and multithreading of current algorithms.
%} \tabularnewline \hline \midrule

%%\ESRa will study real-time object identification in the CMS experiment for HEP and industry, receive training in state-of-the art ML and AI methods, and use these to improve mobile image processing and measure Higgs and $Z$ boson decays to b-quark pairs.

%\ESRb will study RTA in both HEP and industry, be trained in state-of-the art ML and AI methods, and use these to improve ATLAS track reconstruction and monitoring of industrial machinery. \ESRb will work on industrial production chains in IoT-ready plants, collecting and aggregating sensor data in real-time, and forecasting their behavior. 

%\ESRc will be trained in algorithm optimization for highly parallel computing architectures in both HEP and industry, deploy this to improve the performance of both ATLAS RTA and commercial investment code, and search for LLPs with the first Run 3 ATLAS data. 

%\ESRd will develop unified real-time selections and models in HEP and industry, and be trained in state of the art AI and ML methods and algorithms for real-time applications.

%\ESRe will study deep learning (DL) RTAs of complex systems, in both HEP and industry. 

%\ESRf will be trained in the real-time reconstruction of trajectories and images in both HEP and industry, and apply them to the reconstruction and analysis of particle trajectories in ATLAS as well as GPS data processing. 

%\ESRg hybrid architectures

%\ESRh will study and be trained in advanced ML methods for RTA in both HEP and industry, and apply these methods to the optimization of resources for RTA, searches for LFV, and mobile traffic apps. 

%\ESR13 (IBM) will be trained in the most advanced industrial ML and AI anomaly detection methods, including development for supercomputer systems, and use these in RTA to search for dark matter particles with ATLAS.

%\ESRl (HD ATLAS): noise suppression in real time, FTK
%\ESRn (HD LHCb): dark photon RTA

%\ESRm (KKT) will be trained in state-of-the-art computer vision algorithms based on deep learning and will study how to adapt and specialize them for RTA of videos collected by dashcams (camera on vehicle) installed on \fleetmatics customers.

%\ESRx will contribute to the next generation of AI that bridges a data driven automated learning with human-readable rules in real-time decision making. 

\acronym defines three WPs for the management of the consortium, for training, and for outreach and dissemination, and four research WPs, corresponding to each of the research topics in the previous section. 

In \textbf{\colorbox{orange}{WP3}}\color{black}, we make full use of advanced \textbf{Machine Learning, AI and data analysis} techniques in RTA. This is an emerging field of research that enables real time decision-making, monitoring and discoveries in industry and society. All ESRs will be involved in this WP, including studies of best practices and performance (\ESRh, \ESRi).

In \textbf{\colorbox{yellow}{WP4}}, we will develop algorithm implementations which efficiently use modern computing hardware that is increasingly both highly parallel (\ESRc) and heterogeneous, with CPU-based processing farms complemented by GPU or FPGA co-processors (\ESRf, \ESRg, \ESRl). 
This kind of~\textbf{hybrid architectures} enables faster data processing, and their design requires highly-specialized training which few contemporary researchers in the physical sciences and that the consortium including computer scientists is best placed to deliver. 

In \textbf{\colorbox{green}{WP5}}, we will employ the techniques and tools developed in WP1 and WP2 to enable advanced RTA techniques for \textbf{decision-making}. In HEP, we will develop and commission the upgraded triggers of the main LHC collaborations and use them for physics analysis (all ESRs), while in industry we will improve traffic and fleet safety (\ESRd, \ESRh, \ESRl, \ESRm).
 
Within \textbf{\colorbox{cyan}{WP6}}, we will focus on \textbf{real time monitoring of complex processes and preparations for unexpected discoveries}. This feeds back to the decision-making WP5 through novelty/anomaly detection algorithms (\ESRj) that are understandable and reproducible, in HEP, finance (\ESRx) and predictive maintenance (\ESRb). We will perform physics analysis that is fully based on RTA without the need to save raw data (\ESRa, \ESRd, \ESRn, \ESRl, \ESRk). 

All results will be \textbf{disseminated and communicated} to a variety of audiences within \textbf{\colorbox{violet}{\color{white}WP7\color{black}}}, paying particular attention so that publications, software and documentation are accessible as legacy of the action according to the FAIR principles. 

HEP experiments are by their very nature built around \textbf{training}, as around a third of all LHC collaborations consist of PhD students. The training WP (\textbf{\colorbox{red}{WP2}}) encompasses WP3-7 and combines this collaborative, training-based research culture, with a focus on cutting-edge technologies. 

Throughout this ETN, RTA will provide the common language linking these challenges and disciplines, in research, academia and industry, and will enable each of the WPs to benefit from the progress made in the others.
The WPs are summarized in the Table below, and Fig 1. gives a schematic view of the four research WPs and the main actors within the consortium. 

%\vskip10pt
\begin{center}
\scriptsize
\begin{tabular}{p{10mm}p{30mm}p{35mm}p{5mm}p{5mm}p{30mm}p{13mm}p{18mm}}
\toprule
\pbox{8cm}{WP No.} &
\pbox{8cm}{WP Title} &
%This is the number of the beneficiary in the original order
\pbox{8cm}{\Tstrut Lead \\Beneficiary\\No. / responsible\Bstrut} &  
\pbox{8cm}{Start} &  
\pbox{8cm}{End} & 
\pbox{8cm}{Activity Type} & 
\pbox{8cm}{\Tstrut Lead \\Beneficiary\\Short Name\Bstrut} &  
\pbox{8cm}{ESRs\\Involvem't}\tabularnewline\toprule

\cellcolor{red!70!black} \textbf{\color{white}WP1\color{black}}  & Management & N, Doglioni  & 1 & 48 & Management & \lundentity & - \tabularnewline\hline\midrule
\cellcolor{red} \textbf{\color{white}WP2\color{black}}    & Training   &  N, Sfyrla & 5 & 42 & Recruitment, training & \unigeentity & All \tabularnewline\hline\midrule
\cellcolor{orange} \textbf{\color{black}WP3\color{black}}   & ML, AI and data analysis &  N, Gligorov & 1 & 48 & Research& \cnrs & 1-14 \tabularnewline \hline \midrule
\cellcolor{yellow} \textbf{\color{black}WP4\color{black}}    & Hybrid architectures & N, Lacassagne & 1 & 48 & Research & \sorbonneentity  & \ESRsForWPFourText \tabularnewline \hline \midrule
\cellcolor{green} \textbf{\color{black}WP5\color{black}}   & \makecell[vl]{Real-time decision making} & N, Albrecht/Sopasakis & 1 & 48 & Research & \makecell[vl]{\dortmundentity} & All \tabularnewline\hline \midrule
\cellcolor{cyan} \textbf{\color{black}WP6\color{black}}   & Monitoring \& discoveries & N, de Sainte Marie/Pierini & 1 & 48 & Research& \ibmentity & \ESRsForWPSixText \tabularnewline\hline \midrule 
\cellcolor{violet} \textbf{\color{black}WP7\color{black}}  & Dissemination \& communication of results  & N, Petersen & 1 & 48 & \pbox{8cm}{Dissemination, outreach} & \cern & All \tabularnewline
\bottomrule
\end{tabular}
\end{center}

\begin{center}
\includegraphics[width=0.7\textwidth]{figs/NetworkCompositionCombinedImplementation} %scienceStructure_2.pdf}
\begin{center}\footnotesize \label{fig:implementation}
Figure 1: \acronym implementation strategy and main node expertise.
\end{center}
\normalsize 
\vspace{-2mm}
\end{center}

In \acronym, cutting-edge techniques in ML and hybrid architectures will be assessed and developed by the ESRs. They will be applied to specific use cases to RTA techniques that advance HEP, industry and society, and the results exploited, disseminated and communicated (Sec.~\ref{sec:qualityExploitDissemination}). These steps correspond to the the milestones in Sec.~\label{sub:milestones}. This modus operandi is at the heart of \acronym and ensures the efficiency of its implementation. 

%TODO: shorten this to one sentence, remove the blue
\subsubsection{Originality and innovative aspects of the research program} 
\label{sub:Originality}
%Expand on the state of the art: why is this original, innovative ad timely
%Refer to the state of the art in the first paragraph

Add gender!! We got the benchmark of 40\% with node responsibles

\noindent {\color{blue}{1. Researchers from \acronym learn to process large datasets using novel analysis techniques}.}
%  \begin{wrapfigure}{r}{0.7\textwidth}
%%\begin{figure}{l}{\textwidth}
	%\vspace{12mm}
%	\includegraphics[width=0.7\textwidth]{figs/WPs} %scienceStructure_2.pdf}
%    \vspace{-5mm} 
%	\caption*{Figure 2 : Structure and Research Objectives of the Work Packages.\label{fig:WPs}}
     %\vspace{-5mm} 
%%\begin{figure}	
%\end{wrapfigure}
A large part of the novelty of this proposal is the volume of data which will be processed, comparable to the largest commercial tasks. 
% MLD this is a great sentence but it is just out of place here
%This is true across both academic and non-academic applications:
%an estimated 90$\%$ of generated data is considered too expensive to store\footnote{\href{http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation}{2010 report on Big Data by McKinsey\&Company.}}.
One can compare Facebook and e.g. the LHCb collaboration.
%MLD I switched to numbers because then the comparison is more powerful (also shorter!)
The former processes $O(100)$ petabytes of data per year and spends 500M dollars a year on computing\footnote{Facebook, \href{http://www.datacenterknowledge.com/the-facebook-data-center-faq-page-three/}{The Facebook Data Center FAQ}, 2010.} while the latter processes  $O(1000)$ petabytes of data per year and spends around 7M dollars a year on computing\footnote{Private communication, \href{mailto:peter.clarke@ed.ac.uk}{Prof. Peter Clarke}, University of Edinburgh.}.
The essential difference is that Facebook stores and distributes this data to its users while LHC experiments largely process and then dispose of the data.
%MLD I felt this was not needed to make the above argument. It removes your figure 3 but see my comment from the email. 
%An example of this are the physics searches and measurements performed solely using trigger information.
%In these searches only a small fraction of each event, regardless of whether LHC experiments are able to record it for offline reconstruction or not, is saved for further processing. 
%This overcomes the storage limitations and allows to be more than an order of magnitude more sensitive to certain new particles (e.g. associated to Dark Matter, as in Fig.3).

To achieve and go beyond this, the LHC experiments need a more systematic application of RTA, machine learning and hybrid architectures for HEP. Examples are methods using Deep Learning techniques, which build high-level features from raw data (ESR1 and ESR2), using Recurrent Neural Networks (RNNs), which learn ordered patterns in the data and can be applicable both to tracking and to model paths taken by vehicles (ESR10), and Generative Adversarial Networks (GAN) for anomaly detection techniques (ESR13).
This paradigm shift can be used in industry as well, while the research environment can benefit from a generation of ESRs trained in industrial grade algorithms and tools. 

%\begin{wrapfigure}{r}{0.4\textwidth}
%	\vspace{-4mm}
%\includegraphics[width=0.4\textwidth]{figs/TLA.png} 
%    \vspace{-10mm} 
%    \caption*{Figure 3: Example of the number of events using traditional techniques (blue) compared to a trigger-level analysis (black)
%in the search for new particles. Main analyzers: Boveia, Doglioni, Starovoitov, Dunford. \label{fig:TLA}}
%\vspace{-4mm}
%    \end{wrapfigure}
%    \vspace{-3mm} 
%Real-time analysis, machine learning and hybrid architectures have so far not been systematically
%applied to HEP problems. 
%Moreover, even where toolkits exist for HEP,
%notably \tmva\ or \scikit\, they are little more than a collection
%of individual algorithms applied in an ad-hoc manner.
%The research program of \acronym is developed coherently around 
%four main research questions that are at the forefront of 
%the long-term goals of all major LHC experiments. 
%which work against each other, 
%one generating examples and the other classifying them. 
%The classifying network gets a normal score, while 
%the generating network is scored when it can create an example	
%that escapes the classifier, so to make the classifier more 
%sensitive to non-standard cases as it is essential in RTA
%where rare, non-standard but interesting events risk being lost for good. 

%%CD: possible text about machine learning
%An additional driving challenge for the \acronym research program is the continuous need for improvement of background rejection
%techniques once the data has been taken. The discovery of the Higgs boson in 2012, which lead to the award of a Nobel Prize in Physics, has 
%opened the door to a whole new set of measurements of the properties of this new particle and possible discoveries 
%of deviations from the Standard Model. However, the frequency with which
%the Higgs boson particle is produced is minuscule compared to the rates of the backgrounds yielding the same detector signatures as the Higgs boson.  
%An efficient background rejection is key for both measurements involving the Higgs boson and similarly rare particles. 
%The need for novel techniques that only select the interesting events and reject the background is acute, in high energy physics and in
%commercial applications alike. The first challenge of analyzing events in real-time addressed by \acronym also has similar needs: 
%the current paradigm of triggering on simple features 
%ignores the growing importance of the analysis of raw, unstructured, data across both academia
%and industry. For both issues, a series of techniques known as machine learning, multivariate analysis, and 

\noindent {\color{blue}{2. The \acronym program of searches and measurement could lead to breakthroughs in our understanding of nature}}
Research topics chosen to drive conceptual developments within \acronym have potential to lead to the discovery of new physics beyond the Standard Model, but only RTA techniques enable full exploitation of the LHC dataset. 
%the full statistical reach of the LHC data to be exploited. 
%Only RTA techniques will enable such discoveries. 
ESRs working on physics topics will target common challenges, e.g. when real-time algorithms and reconstruction techniques are not sufficiently advanced to distinguish signal from noise, or when the statistical power of the dataset is not fully exploited if objects are reconstructed with traditional techniques. 
%The advanced tools under design in \acronym
%will allow the real-time systems of LHC 
%experiments to improve coherently. 

% MLD this sudden detailed motivation of the physics seems out of place. This section is supposed to be motivating why the training program is unique. Also the level of detail here is not consistent with other parts. 
Examples include studies of lepton flavor and its conservation and universality in different final states and experiments (ESR 5-7, 9, 11-12), and dark matter mediators and new light particles\footnote{M. Chala et al, \href{http://arxiv.org/abs/1503.05916}{Constraining Dark Sectors with Monojets and Dijets}, JHEP 1507 (2015) 089.}.
The volume of data needed to be sensitive to these rare processes is the perfect testing ground for improved real-time techniques for ESR1 and 8.  
RTAs looking for dark matter mediators in ESR13 and 15, and the search for new particles of ESR3 and 4 using ML have never been performed at the LHC, nor have generic searches for rare new phenomena as in ESR2.
Measurements in ESR2 and 15 precisely probe the SM in the Higgs and heavy ions sectors. 

The choice of physics topics that all need RTA to achieve beyond-state-of-the-art results ensures advancement in detector development and contribution to major advances in key analyses of LHC data.
%paving the way to understand the main questions of our universe. 

\noindent {\color{blue}{3. ESRs in \acronym deploy and disseminate their research at a unique time for particle physics}.}
As highlighted by the HEP Software Foundation Whitepaper$^{4}$, the period 2019-2023 is ideal for this R\&D in HEP, as it is a time of transition between LHC data taking periods that will be necessary to prepare for an upgrade of the LHC accelerator where the amount of data delivered will make RTA techniques  the key to pursue the physics programs of the four main LHC experiments.
The systematic optimization of HEP experiments by \acronym will boost the performance of the current and planned upgrades of the CERN based accelerator experiments.
Furthermore, the developed toolkits will be advertised at international conferences and thus the developed methods will shape the online event selection of all future HEP experiments. 
%The next LHC run (Run-3) will be starting in 2021 and the following one (Run-4) will begin in 2026.  
%Therefore, the 2020-2024 period is an ideal time for performing trigger upgrades and commissioning them with data, following the LHC upgrades that will deliver a much larger dataset than what is currently available. 

\noindent {\color{blue}{4. \acronym researchers develop techniques and infrastructures that can be exploited in industrial applications, as well as in HEP}.}
The close links of the research institutes of the consortium with the industry partners means that the ESRs will directly drive the development of novel industrial products, while also bringing professional methods of data mining that are exercised in large companies into the academic environment. 
Most modern methods applied in research can in this way be transferred more easily to industry applications in automotive traffic optimization (ESR1-2, 6, 8) and analysis of sensor data for IoT and medical applications (ESR3-4, 9, 11, 13, 15).
%JA: not , 14 (??)) 
The proposed algorithms and the use of ML methods on these scales of data are novel to both HEP and industry. 
By exposing industry-grade methods to the volume and complexity of HEP data, we will stimulate their development in a complementary way for the benefit of industry.


