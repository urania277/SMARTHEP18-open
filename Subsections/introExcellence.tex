%context
\vspace{-2mm}

The volume of data available to research and industry is ever-increasing. 
This acceleration in data collection is not always matched by a comparable acceleration in data storage and data utilization. 
This poses a \textbf{problem for the traditional paradigm of first collecting data, then analyzing it}. 
%\footnote{G. Santucci, \href{http://cordis.europa.eu/fp7/ict/enet/documents/publications/iot-between-the-internet-revolution.pdf}{The Internet of Things: Between the Revolution of the Internet and the Metamorphosis of Objects}, FP7 document}. 
If the space to store the data is limited, then data has to be discarded or not recorded at all. 
Moreover, most of the data produced is recorded and stored without ever being analyzed, and loses value if insight is not extracted in a short period of time\footnote{W. Riess, IBM Zurich, Nanoscience Colloquium at Lund University titled "The Future of Computing", 17/05/2018}. 

At the experiments at the Large Hadron Collider at CERN, information on fundamental components of matter from proton-proton collisions is provided up to 30 million times per second. 
A decision on whether a collision "event" is interesting must be made in the order of milliseconds, and novel techniques are needed in order to extract information that would otherwise be discarded. 
For commercial applications such as mobile apps for fleet control and transport optimization, IoT sensors and fraud detection, the amount and complexity of the data grows while the resources and time to take a decision using that information do not scale accordingly -- industry faces a similar challenge as High Energy Physics (HEP). 

In order to make the most of the available information in a cost-effective way, and to reduce the time between data collection and insight into the results to inform subsequent decisions, data-taking and data-analysis need to be fast and efficient. 
This is why \acronym ({\color{blue}\textbf{S}}ynergies between {\color{blue}\textbf{M}}ultivariate {\color{blue}\textbf{A}}nalysis, {\color{blue}\textbf{R}}eal {\color{blue}\textbf{T}}ime analysis and {\color{blue}\textbf{H}}ybrid architectures for {\color{blue}\textbf{E}}vent {\color{blue}\textbf{P}}rocessing) is an interdisciplinary ETN aiming to train Early Stage Researchers (ESRs) on \textbf{Real Time Analysis (RTA)} techniques, where \textbf{data collection and analysis are nearly simultaneous}, using novel software and hardware tools. 
Within \acronym, the ESRs will employ machine learning (ML) and multivariate techniques as well as artificial intelligence (AI) to speed up the reconstruction and the analysis of the data, introducing novel algorithms that improve reproducibility and accountability of real-time decision making. 
The ESRs will become experts in novel hybrid software/hardware solutions, where data can be collected and reconstructed on the same platform. 

The \acronym \textbf{ESRs will be trained} by a consortium of successful researchers in \textbf{High Energy Physics and Computer Science} (including 5 holders of ERC StG and CoG, as well as recipients of multiple national grants, and includes a mentoring group of experienced scientists worldwide) and entrepreneurs from industries ranging from multinational to start-up. 
These scientists, professionals and entrepreneurs have begun their collaboration within \acronym and will continue working together beyond this action with a commitment to follow the future careers of the scientists they train. 
The ESRs will \textbf{learn transferrable skills} from the exposure to research, industry and society and network events. 
They will be \textbf{prepared for careers in industry or science} and \textbf{contribute to European growth} from their hands-on work on concrete commercial and research deliverables, such as 
%KKT - IT
mobile apps using ML for efficient vehicle fleet control, 
%Ximantis - SWE / Point8 - DE
optimized algorithms for traffic and public transport predictions,
%Lightbox
software for sensors that will monitor industrial production, 
%IBM - FR
accountable and understandable algorithms for fraud detection and decision-making in finance,
%research
as well as cutting edge computing technologies and algorithms necessary for the detection and measurement of fundamental particles and for the discovery of new physics phenomena. 
